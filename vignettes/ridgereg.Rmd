---
title: "ridgereg"
author: "Erika Anderskar and Georgia Mushe"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Lab4}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---
This is an example of the caret package. 


```{r setup, include=FALSE}
library(mlbench)
library(caret)
library(Lab4)
```

# Part 1
The caret package is used to partition the data into a training and test data set.
```{r TrainTest}
data(BostonHousing)

inTrain <- createDataPartition(y = BostonHousing$medv, times=1, p = 0.75,list = FALSE)

training <- BostonHousing[inTrain,]
test <- BostonHousing[-inTrain,]
```

#Part 2

With the caret package two models are fitted to the BostonHousing data.

```{r }
BostonFit1 <- train(medv ~ ., data = training,method = "lm")

BostonFit2 <- train(medv ~ ., data = training,method = "leapForward")

BostonFit1$results
BostonFit2$results

```

#Part 3

The linear regression looks like a better option in this case since the $R^2$ is higher and the Root Mean Squared Error is lower. 

#Part 4 and 5

We use our ridgereg function from the "Lab4" package to fit a ridgeregression to BostonHousing data. Seven lambda values are tested. To find the optimal lambda a 10-fold cross-validation is used.

```{Fit3}
ridgereg_ <- list(type="Regression", library="Lab4", prob=NULL)

ridgereg_$parameters <- getModelInfo("ridge")$ridge$parameters

ridgereg_$grid <- function(x,y,len=NULL, search= "grid"){
  out <- data.frame(lambda=c(0.1,0.2,0.5,1,1.5,2,2.5))
}
ridgereg_$fit <- function (x, y, wts, param, lev, last, classProbs, ...) {
  dat <- if (is.data.frame(x)) 
    x
  else as.data.frame(x)
  dat$.outcome <- y
  out <- ridgereg$new(.outcome ~ ., data = dat, lambda=param$lambda, ...)
  out
}

ridgereg_$predict <- function (modelFit, newdata, submodels = NULL) {
  if (!is.data.frame(newdata)) 
    newdata <- as.data.frame(newdata)
    newdata <- scale(newdata)
    modelFit$predict(newdata)
}

control <- trainControl(method = "repeatedcv",
                        number=10,
                        repeats = 10)

BostonFit_3 <<- train(medv ~ ., data=training, method=ridgereg_, trControl=control)


```

 
The optimal lambda is lambda=2.5.

#Part 6
The three models are compared with following code:
```{r, eval=FALSE}
BostonFit1_test <- predict(BostonFit1, test)
postResample(pred=BostonFit1_test, obs=test$medv)

BostonFit2_test <- predict(BostonFit2, test)
postResample(pred=BostonFit2_test, obs=test$medv)

BostonFit3_test <- predict(BostonFit_3, test)
postResample(pred=BostonFit3_test, obs=test$medv)
```
The output shows that the linear regression model performs best.











